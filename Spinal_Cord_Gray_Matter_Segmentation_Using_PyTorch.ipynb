{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spinal Cord Gray Matter Segmentation Using PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhoshk/ml/blob/master/Spinal_Cord_Gray_Matter_Segmentation_Using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ocm6A50JLtO",
        "colab_type": "text"
      },
      "source": [
        "# Spinal Cord Gray Matter Segmentation Using PyTorch\n",
        "In this notebook we are going to explore a medical imaging open-source library known as [MedicalTorch](https://github.com/perone/medicaltorch), which was built on top of PyTorch. The purpose of this tutorial is to show you the basic functionalities of the tools offered in MedicalTroch, such as pre-processing of images, transformations, and data loaders. With the tools you will be able to explore and preprocess MRI-based datasets that contain images such as shown in the image below. As you progress in the tutorial you shall begin to appreciate the capabilities and possibilities of the medicaltorch library for medical imaging AI applications. \n",
        "\n",
        "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vQmKTAQEz_zoomKa0HchswOUunHQTO3gDoH_VnXfcZdl8N3H0L-aGvM9GqQiiLi-bL0ME4-IoU6cv7g/pub?w=928&h=499)\n",
        "\n",
        "Image credit: [Perone et al.,2017](https://arxiv.org/pdf/1710.01269.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8sWyzGOJ02-",
        "colab_type": "text"
      },
      "source": [
        "## Installing Libraries\n",
        "Below we will show you how to install the medicaltorch library along with some other libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzzQn1BQpHHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl \n",
        "!pip3 install torchvision\n",
        "!pip install medicaltorch\n",
        "!pip3 install numpy==1.14.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEu5fBCvpMqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkvGLISDp0Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tel5eU7DDF39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "print(np.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh8sG3huKAK_",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Data From Google Drive\n",
        "I have stored the MRI images on my personal Google Drive, but you may request the data from the [GM SC Challenge](http://niftyweb.cs.ucl.ac.uk/challenge/index.php#citation) website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dEjMdM0sKJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy4iVLD2K0QO",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries and Let's Get Started!\n",
        "Let's import the necessary libraries includinf the utility functions from the `medicaltorch` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD3mbIc8tzed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from medicaltorch import datasets as mt_datasets\n",
        "from medicaltorch import models as mt_models\n",
        "from medicaltorch import transforms as mt_transforms\n",
        "from medicaltorch import losses as mt_losses\n",
        "from medicaltorch import metrics as mt_metrics\n",
        "from medicaltorch import filters as mt_filters\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import autograd, optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "cudnn.benchmark = True\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z836WKgLCXC",
        "colab_type": "text"
      },
      "source": [
        "### Data Exploration\n",
        "Before we do any modeling stuff, let's investigate our data first. Let's look at one sample (MRI image) from the dataset. We will see the preprocessing module `mt_datasets.SegmentationPair2D` which is used to read and format the data in a way that we can better explore it in our environment. See the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGxyNTVItVYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT_DIR_GMCHALLENGE = \"/gdrive/My Drive/DAIR RESOURCES/PyTorch/medical_imaging/train/\"\n",
        "mri_input_filename = os.path.join(ROOT_DIR_GMCHALLENGE,\n",
        "                                          'site1-sc01-image.nii.gz')\n",
        "mri_gt_filename = os.path.join(ROOT_DIR_GMCHALLENGE,\n",
        "                                       'site1-sc01-mask-r1.nii.gz')\n",
        "\n",
        "pair = mt_datasets.SegmentationPair2D(mri_input_filename, mri_gt_filename)\n",
        "slice_pair = pair.get_pair_slice(0)\n",
        "input_slice = slice_pair[\"input\"]\n",
        "gt_slice = slice_pair[\"gt\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40n8eVQkwlCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(input_slice.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5zCh2ueML0D",
        "colab_type": "text"
      },
      "source": [
        "As you can see above, images are 100 X 100 dimensions. We can also view the image in our environment using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF7Teg-EtuEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = input_slice\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTMQ2HN4y-p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = gt_slice\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-pnIwP2MXtC",
        "colab_type": "text"
      },
      "source": [
        "The upper image shows an *in vivo axial-slice* sample while the lower image shows the spinal cord gray matter segmentation. Intuitively, the dataset contains the necessary information to conduct gray matter segmentation using deep learning techniques such as encode/decoder frameworks and deep convolutional neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUFHeSBwzgZ7",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch Data Loaders\n",
        "Let's try to use the data loaders offered in medicaltorch to test the transformation functions. PyTorch offers a \"transforms\" module that helps us to apply any transformations on our data that we may wish, this is by stacking operations a seen below. First we resample the dataset so that all samples are of the same size and then apply a cropping area, followed by a tensor type transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wUOf_CMzg6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transformer\n",
        "composed_transform = transforms.Compose([\n",
        "            mt_transforms.Resample(0.25, 0.25),\n",
        "            mt_transforms.CenterCrop2D((200, 200)),\n",
        "            mt_transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaufm7yizjca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "train_dataset = mt_datasets.SCGMChallenge2DTrain(root_dir=ROOT_DIR_GMCHALLENGE, transform=composed_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c1ZteF4zlPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7aY-hczOxuN",
        "colab_type": "text"
      },
      "source": [
        "You can see an example below of how a raw image was converted to a tensor format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q89Awl_9z3yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample of the training dataset\n",
        "train_dataset[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ySWB4PpPGFE",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the image loaded and transformed into tensors, we provided it to the PyTorch native Dataloader function to do its magic. The DataLoader below basically creates mini-batches and shuffles them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qeVpO6kz5s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch data loader\n",
        "dataloader = DataLoader(train_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=4,\n",
        "                        collate_fn=mt_datasets.mt_collate)\n",
        "minibatch = next(iter(dataloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-IZvi2qz7jG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check minibatch size\n",
        "minibatch['input'].size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9R5Ju2Pl3B",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Batches\n",
        "Below we show you how to visualize batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O0TyYdmz97G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for item in minibatch['input']:\n",
        "    plt.imshow(item.squeeze(0))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKo7LpP40KkA",
        "colab_type": "text"
      },
      "source": [
        "## Constructing The Segmentation Model\n",
        "We saw the images above, now we want to build the gray matter segmentation model with the MRI spinal cord images provided above. Let's define a helper function that helps to decide the final predictions of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI-nlqfA0Mlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def threshold_predictions(predictions, thr=0.999):\n",
        "    thresholded_preds = predictions[:]\n",
        "    low_values_indices = thresholded_preds < thr\n",
        "    thresholded_preds[low_values_indices] = 0\n",
        "    low_values_indices = thresholded_preds >= thr\n",
        "    thresholded_preds[low_values_indices] = 1\n",
        "    return thresholded_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSnfGfkNbV2z",
        "colab_type": "text"
      },
      "source": [
        "And here are all the transformations to both the training and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzHLV9Rg0PUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "        mt_transforms.Resample(0.25, 0.25),\n",
        "        mt_transforms.CenterCrop2D((200, 200)),\n",
        "        mt_transforms.ElasticTransform(alpha_range=(28.0, 30.0),\n",
        "                                       sigma_range=(3.5, 4.0),\n",
        "                                       p=0.3),\n",
        "        mt_transforms.RandomAffine(degrees=4.6,\n",
        "                                   scale=(0.98, 1.02),\n",
        "                                   translate=(0.03, 0.03)),\n",
        "        mt_transforms.RandomTensorChannelShift((-0.10, 0.10)),\n",
        "        mt_transforms.ToTensor(),\n",
        "        mt_transforms.NormalizeInstance(),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "        mt_transforms.Resample(0.25, 0.25),\n",
        "        mt_transforms.CenterCrop2D((200, 200)),\n",
        "        mt_transforms.ToTensor(),\n",
        "        mt_transforms.NormalizeInstance(),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiBw-mus0RSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### training dataset with 80/20 split\n",
        "TRAIN_ROOT_DIR_GMCHALLENGE = \"/gdrive/My Drive/DAIR RESOURCES/PyTorch/medical_imaging/train/\"\n",
        "\n",
        "gmdataset_train = mt_datasets.SCGMChallenge2DTrain(root_dir=TRAIN_ROOT_DIR_GMCHALLENGE,\n",
        "                                                   subj_ids=range(1, 9),\n",
        "                                                   transform=train_transform,\n",
        "                                                   slice_filter_fn=mt_filters.SliceFilter())\n",
        "\n",
        "gmdataset_val = mt_datasets.SCGMChallenge2DTrain(root_dir=TRAIN_ROOT_DIR_GMCHALLENGE,\n",
        "                                                 subj_ids=range(9, 11),\n",
        "                                                 transform=val_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5dB05YU0S8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(gmdataset_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOJiyUKr0uOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(gmdataset_train, batch_size=16,\n",
        "                          shuffle=True, pin_memory=True,\n",
        "                          collate_fn=mt_datasets.mt_collate,\n",
        "                          num_workers=1)\n",
        "\n",
        "val_loader = DataLoader(gmdataset_val, batch_size=16,\n",
        "                        shuffle=True, pin_memory=True,\n",
        "                        collate_fn=mt_datasets.mt_collate,\n",
        "                        num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xvvubINbeo4",
        "colab_type": "text"
      },
      "source": [
        "### Model and Parameters\n",
        "Below we declare our model and parameters. Note that we are using GPU in this notebook. Also note that the model used below refers to the U-net convolutional-based architecture proposed by [Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597), which essentially aggregates semantic information to perform the segmentation. See a figure of the U-net framework below. You can also refer to the medicaltorch [API documentation](https://medicaltorch.readthedocs.io/en/stable/modules.html#module-medicaltorch.models) for more available state-of-the-art implementations. \n",
        "\n",
        "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vT2miqwBsJpm9vX2lH7GRJaMWw3ym9Ld3MNY-10rpKIQJoXvfsRbIu1OpndIn4BJqYUtpq3wZcwmS9v/pub?w=921&h=624)\n",
        "\n",
        "Image Credit: [Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqL4S4i0v_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = mt_models.Unet(drop_rate=0.4, bn_momentum=0.1)\n",
        "model.cuda()\n",
        "num_epochs = 10\n",
        "initial_lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu2HXF7sbtm3",
        "colab_type": "text"
      },
      "source": [
        "Some helper functions to produce the desired metrics for the model such as accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4gGMehXJybI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numeric_score(prediction, groundtruth):\n",
        "    FP = np.float(np.sum((prediction == 1) & (groundtruth == 0)))\n",
        "    FN = np.float(np.sum((prediction == 0) & (groundtruth == 1)))\n",
        "    TP = np.float(np.sum((prediction == 1) & (groundtruth == 1)))\n",
        "    TN = np.float(np.sum((prediction == 0) & (groundtruth == 0)))\n",
        "    return FP, FN, TP, TN \n",
        "  \n",
        "def accuracy(prediction, groundtruth):\n",
        "    FP, FN, TP, TN = numeric_score(prediction, groundtruth)\n",
        "    N = FP + FN + TP + TN\n",
        "    accuracy = np.divide(TP + TN, N)\n",
        "    return accuracy * 100.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8cOB2DcEWt",
        "colab_type": "text"
      },
      "source": [
        "### Training \n",
        "Now we finally train the model for spinal cord gray matter segmentation. We report the training and testing accuracy below and train for 10 epochs only. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psr9TSoI0x8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in tqdm(range(1, num_epochs+1)):\n",
        "    start_time = time.time()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    lr = scheduler.get_lr()[0]\n",
        "\n",
        "    model.train()\n",
        "    train_loss_total = 0.0\n",
        "    num_steps = 0\n",
        "    \n",
        "    ### Training\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        input_samples, gt_samples = batch[\"input\"], batch[\"gt\"]\n",
        "\n",
        "        var_input = input_samples.cuda()\n",
        "        var_gt = gt_samples.cuda(async=True)\n",
        "\n",
        "        preds = model(var_input)\n",
        "\n",
        "        loss = mt_losses.dice_loss(preds, var_gt)\n",
        "        train_loss_total += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        num_steps += 1\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            grid_img = vutils.make_grid(input_samples,\n",
        "                                        normalize=True,\n",
        "                                        scale_each=True)\n",
        "            \n",
        "\n",
        "            grid_img = vutils.make_grid(preds.data.cpu(),\n",
        "                                        normalize=True,\n",
        "                                        scale_each=True)\n",
        "            \n",
        "\n",
        "            grid_img = vutils.make_grid(gt_samples,\n",
        "                                        normalize=True,\n",
        "                                        scale_each=True)\n",
        "   \n",
        "    \n",
        "    train_loss_total_avg = train_loss_total / num_steps\n",
        "    model.eval()\n",
        "    val_loss_total = 0.0\n",
        "    num_steps = 0\n",
        "    train_acc  = accuracy(preds.cpu().detach().numpy(), \n",
        "                          var_gt.cpu().detach().numpy())\n",
        "    \n",
        "    metric_fns = [mt_metrics.dice_score,\n",
        "                  mt_metrics.hausdorff_score,\n",
        "                  mt_metrics.precision_score,\n",
        "                  mt_metrics.recall_score,\n",
        "                  mt_metrics.specificity_score,\n",
        "                  mt_metrics.intersection_over_union,\n",
        "                  mt_metrics.accuracy_score]\n",
        "\n",
        "    metric_mgr = mt_metrics.MetricManager(metric_fns)\n",
        "            \n",
        "    ### Validating\n",
        "    for i, batch in enumerate(val_loader):\n",
        "        input_samples, gt_samples = batch[\"input\"], batch[\"gt\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            var_input = input_samples.cuda()\n",
        "            var_gt = gt_samples.cuda(async=True)\n",
        "\n",
        "            preds = model(var_input)\n",
        "            loss = mt_losses.dice_loss(preds, var_gt)\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "        # Metrics computation\n",
        "        gt_npy = gt_samples.numpy().astype(np.uint8)\n",
        "        gt_npy = gt_npy.squeeze(axis=1)\n",
        "\n",
        "        preds = preds.data.cpu().numpy()\n",
        "        preds = threshold_predictions(preds)\n",
        "        preds = preds.astype(np.uint8)\n",
        "        preds = preds.squeeze(axis=1)\n",
        "\n",
        "        metric_mgr(preds, gt_npy)\n",
        "\n",
        "        num_steps += 1\n",
        "        \n",
        "    metrics_dict = metric_mgr.get_results()\n",
        "    metric_mgr.reset()\n",
        "    val_loss_total_avg = val_loss_total / num_steps\n",
        "   \n",
        "    print('\\nTrain loss: {:.4f}, Training Accuracy: {:.4f} '.format(train_loss_total_avg, train_acc))\n",
        "    print('Val Loss: {:.4f}, Validation Accuracy: {:.4f} '.format(val_loss_total_avg, metrics_dict[\"accuracy_score\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEmtOczpglkP",
        "colab_type": "text"
      },
      "source": [
        "## Final Words\n",
        "In summary, you learned how to process MRI image scans using a neat and powerful tool known as medicaltorch. In addition, you learned about how to preprocess, prepare and load the data using medicaltorch and PyTorch's build-in DataLoader module. Finally, you trained a model based on convolutional neural networks to conduct spinal cord gray matter segmentation. Feel free to explore more of the utility function provided in the medicaltorch API and explore different types of datasets. Until next time!"
      ]
    }
  ]
}